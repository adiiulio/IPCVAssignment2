{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNBgGYg_lpVN"
   },
   "source": [
    "# Assignment Module 2: Product Classification\n",
    "\n",
    "The goal of this assignment is to implement a neural network that classifies smartphone pictures of products found in grocery stores. The assignment will be divided into two parts: first, you will be asked to implement from scratch your own neural network for image classification; then, you will fine-tune a pretrained network provided by PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVTQUJ4uYH1w"
   },
   "source": [
    "## Preliminaries: the dataset\n",
    "\n",
    "The dataset you will be using contains natural images of products taken with a smartphone camera in different grocery stores:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Granny-Smith.jpg\" width=\"150\">\n",
    "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Pink-Lady.jpg\" width=\"150\">\n",
    "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Lemon.jpg\" width=\"150\">\n",
    "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Banana.jpg\" width=\"150\">\n",
    "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Vine-Tomato.jpg\" width=\"150\">\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Yellow-Onion.jpg\" width=\"150\">\n",
    "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Green-Bell-Pepper.jpg\" width=\"150\">\n",
    "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Arla-Standard-Milk.jpg\" width=\"150\">\n",
    "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Oatly-Natural-Oatghurt.jpg\" width=\"150\">\n",
    "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Alpro-Fresh-Soy-Milk.jpg\" width=\"150\">\n",
    "</p>\n",
    "\n",
    "The products belong to the following 43 classes:\n",
    "```\n",
    "0.  Apple\n",
    "1.  Avocado\n",
    "2.  Banana\n",
    "3.  Kiwi\n",
    "4.  Lemon\n",
    "5.  Lime\n",
    "6.  Mango\n",
    "7.  Melon\n",
    "8.  Nectarine\n",
    "9.  Orange\n",
    "10. Papaya\n",
    "11. Passion-Fruit\n",
    "12. Peach\n",
    "13. Pear\n",
    "14. Pineapple\n",
    "15. Plum\n",
    "16. Pomegranate\n",
    "17. Red-Grapefruit\n",
    "18. Satsumas\n",
    "19. Juice\n",
    "20. Milk\n",
    "21. Oatghurt\n",
    "22. Oat-Milk\n",
    "23. Sour-Cream\n",
    "24. Sour-Milk\n",
    "25. Soyghurt\n",
    "26. Soy-Milk\n",
    "27. Yoghurt\n",
    "28. Asparagus\n",
    "29. Aubergine\n",
    "30. Cabbage\n",
    "31. Carrots\n",
    "32. Cucumber\n",
    "33. Garlic\n",
    "34. Ginger\n",
    "35. Leek\n",
    "36. Mushroom\n",
    "37. Onion\n",
    "38. Pepper\n",
    "39. Potato\n",
    "40. Red-Beet\n",
    "41. Tomato\n",
    "42. Zucchini\n",
    "```\n",
    "\n",
    "The dataset is split into training (`train`), validation (`val`), and test (`test`) set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pdrmJRnJPd8"
   },
   "source": [
    "The following code cells download the dataset and define a `torch.utils.data.Dataset` class to access it. This `Dataset` class will be the starting point of your assignment: use it in your own code and build everything else around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-11T14:38:14.579828Z",
     "iopub.status.busy": "2024-12-11T14:38:14.579352Z",
     "iopub.status.idle": "2024-12-11T14:38:47.860259Z",
     "shell.execute_reply": "2024-12-11T14:38:47.858901Z",
     "shell.execute_reply.started": "2024-12-11T14:38:14.579787Z"
    },
    "id": "POMX_3x-_bZI",
    "outputId": "4b884e9f-3452-4da4-db03-65e816f8fca9",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'GroceryStoreDataset' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/marcusklasson/GroceryStoreDataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:38:47.863281Z",
     "iopub.status.busy": "2024-12-11T14:38:47.862841Z",
     "iopub.status.idle": "2024-12-11T14:38:47.870934Z",
     "shell.execute_reply": "2024-12-11T14:38:47.869714Z",
     "shell.execute_reply.started": "2024-12-11T14:38:47.863236Z"
    },
    "id": "hiF8xGEYlsu8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List, Tuple\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:38:47.872796Z",
     "iopub.status.busy": "2024-12-11T14:38:47.872297Z",
     "iopub.status.idle": "2024-12-11T14:38:47.890664Z",
     "shell.execute_reply": "2024-12-11T14:38:47.888544Z",
     "shell.execute_reply.started": "2024-12-11T14:38:47.872741Z"
    },
    "id": "jROSO2qVDxdD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GroceryStoreDataset(Dataset):\n",
    "\n",
    "    def __init__(self, split: str, transform=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.root = Path(\"/content/GroceryStoreDataset/dataset\")\n",
    "        self.split = split\n",
    "        self.paths, self.labels = self.read_file()\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[Tensor, int]:\n",
    "        img = Image.open(self.root / self.paths[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def read_file(self) -> Tuple[List[str], List[int]]:\n",
    "        paths = []\n",
    "        labels = []\n",
    "\n",
    "        with open(self.root / f\"{self.split}.txt\") as f:\n",
    "            for line in f:\n",
    "                # path, fine-grained class, coarse-grained class\n",
    "                path, _, label = line.replace(\"\\n\", \"\").split(\", \")\n",
    "                paths.append(path), labels.append(int(label))\n",
    "\n",
    "        return paths, labels\n",
    "\n",
    "    def get_num_classes(self) -> int:\n",
    "        return max(self.labels) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBch3dpwNSsW"
   },
   "source": [
    "## Part 1: design your own network\n",
    "\n",
    "Your goal is to implement a convolutional neural network for image classification and train it on `GroceryStoreDataset`. You should consider yourselves satisfied once you obtain a classification accuracy on the **validation** split of **around 60%**. You are free to achieve that however you want, except for a few rules you must follow:\n",
    "\n",
    "- You **cannot** simply instantiate an off-the-self PyTorch network. Instead, you must construct your network as a composition of existing PyTorch layers. In more concrete terms, you can use e.g. `torch.nn.Linear`, but you **cannot** use e.g. `torchvision.models.alexnet`.\n",
    "\n",
    "- Justify every *design choice* you make. Design choices include network architecture, training hyperparameters, and, possibly, dataset preprocessing steps. You can either (i) start from the simplest convolutional network you can think of and add complexity one step at a time, while showing how each step gets you closer to the target ~60%, or (ii) start from a model that is already able to achieve the desired accuracy and show how, by removing some of its components, its performance drops (i.e. an *ablation study*). You can *show* your results/improvements however you want: training plots, console-printed values or tables, or whatever else your heart desires: the clearer, the better.\n",
    "\n",
    "Don't be too concerned with your network performance: the ~60% is just to give you an idea of when to stop. Keep in mind that a thoroughly justified model with lower accuracy will be rewarded **more** points than a poorly experimentally validated model with higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_n7OMjXBaDiy"
   },
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:38:47.894407Z",
     "iopub.status.busy": "2024-12-11T14:38:47.893948Z",
     "iopub.status.idle": "2024-12-11T14:38:47.910558Z",
     "shell.execute_reply": "2024-12-11T14:38:47.909209Z",
     "shell.execute_reply.started": "2024-12-11T14:38:47.894366Z"
    },
    "id": "W0XqMhoDaDiz",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTJ2g6w3aDiz"
   },
   "source": [
    "### data preprocessing and augmentation\n",
    "In this first step we transform both the train and the validation sets, this is done because we want to do some augmentation in order to reduce the bias of the network.\n",
    "- at first the images are resized to 64x64 in order to ensure uniform input size\n",
    "- then we have the data augmentation (of course, only for the training set) with RandomHorizontalFlip and RandomRotation\n",
    "- finally we normalize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:38:47.912615Z",
     "iopub.status.busy": "2024-12-11T14:38:47.912140Z",
     "iopub.status.idle": "2024-12-11T14:38:47.925379Z",
     "shell.execute_reply": "2024-12-11T14:38:47.924076Z",
     "shell.execute_reply.started": "2024-12-11T14:38:47.912568Z"
    },
    "id": "kMGmy1mlaDiz",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Data augmentation and normalization\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "_5APoSYSTePY"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:38:47.927843Z",
     "iopub.status.busy": "2024-12-11T14:38:47.927426Z",
     "iopub.status.idle": "2024-12-11T14:38:47.959544Z",
     "shell.execute_reply": "2024-12-11T14:38:47.958324Z",
     "shell.execute_reply.started": "2024-12-11T14:38:47.927805Z"
    },
    "id": "JI5KKJTCaDi0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Define the device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the datasets\n",
    "train_dataset = GroceryStoreDataset(split='train', transform=train_transforms)\n",
    "val_dataset = GroceryStoreDataset(split='val', transform=val_transforms)\n",
    "test_dataset = GroceryStoreDataset(split='test', transform=test_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAFgYzQUaDi0"
   },
   "source": [
    "### plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:38:47.961460Z",
     "iopub.status.busy": "2024-12-11T14:38:47.961064Z",
     "iopub.status.idle": "2024-12-11T14:38:47.972366Z",
     "shell.execute_reply": "2024-12-11T14:38:47.970931Z",
     "shell.execute_reply.started": "2024-12-11T14:38:47.961425Z"
    },
    "id": "clgRrpT3aDi0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_accuracy(train_acc, val_acc):\n",
    "\n",
    "    epochs = range(1, len(train_acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs, train_acc, 'b', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(train_loss, val_loss):\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs, train_loss, 'b', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGCeS6BraDi0"
   },
   "source": [
    "### definition of training function\n",
    "Here we define the training function. It takes as input the model, the train and val loader, as well as the criterion, optimizer, scheduler, number of epochs and patience. The total number of epoch is set to 20, although if there is no improvement after 5 steps the model stops training. At each epoch, we train on the train set and then evaluate the performance on the validation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:38:47.974040Z",
     "iopub.status.busy": "2024-12-11T14:38:47.973610Z",
     "iopub.status.idle": "2024-12-11T14:38:47.990674Z",
     "shell.execute_reply": "2024-12-11T14:38:47.989309Z",
     "shell.execute_reply.started": "2024-12-11T14:38:47.973964Z"
    },
    "id": "AAbfRr9gaDi0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model_with_early_stopping_and_scheduler(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=20, patience=5):\n",
    "    \"\"\"\n",
    "    Train the model on the training set and evaluate it on the validation set, with early stopping and a learning rate scheduler.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training dataset.\n",
    "        val_loader (DataLoader): DataLoader for the validation dataset.\n",
    "        criterion (nn.Module): Loss function used to calculate training and validation loss.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for model weight updates.\n",
    "        scheduler (torch.optim.lr_scheduler): Learning rate scheduler to adjust the learning rate during training.\n",
    "        num_epochs (int, optional): Maximum number of training epochs. Default is 20.\n",
    "        patience (int, optional): Number of consecutive epochs without validation accuracy improvement before early stopping. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        model (nn.Module): The trained model with the best weights (based on validation accuracy).\n",
    "        train_loss_arr (list): List of training loss values for each epoch.\n",
    "        train_acc_arr (list): List of training accuracy values for each epoch.\n",
    "        val_loss_arr (list): List of validation loss values for each epoch.\n",
    "        val_acc_arr (list): List of validation accuracy values for each epoch.\n",
    "    \"\"\"\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "\n",
    "    train_loss_arr = []\n",
    "    train_acc_arr = []\n",
    "    val_loss_arr = []\n",
    "    val_acc_arr = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "\n",
    "        train_loss_arr.append(epoch_loss)\n",
    "        train_acc_arr.append(epoch_acc)\n",
    "\n",
    "        print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = val_corrects.double() / len(val_loader.dataset)\n",
    "\n",
    "        val_loss_arr.append(val_loss)\n",
    "        val_acc_arr.append(val_acc)\n",
    "\n",
    "        print(f'Validation Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "\n",
    "        # Check if early stopping condition is met\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            early_stop = True\n",
    "            break\n",
    "\n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    # Restore best model weights\n",
    "    print(f\"Best val Acc: {best_acc:.4f}\")\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, train_loss_arr, train_acc_arr, val_loss_arr, val_acc_arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ct1hbRVaDi1"
   },
   "source": [
    "### definition of evaluation function\n",
    "This function takes as input the model, the test loader and the criterion. It calculates the average test loss and the test accuracy by comparing the computed labels with the true ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:38:47.992608Z",
     "iopub.status.busy": "2024-12-11T14:38:47.992248Z",
     "iopub.status.idle": "2024-12-11T14:38:48.010258Z",
     "shell.execute_reply": "2024-12-11T14:38:48.009042Z",
     "shell.execute_reply.started": "2024-12-11T14:38:47.992574Z"
    },
    "id": "T87fGfFyaDi1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model_on_test_set(model, test_loader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model to be evaluated.\n",
    "        test_loader (DataLoader): DataLoader for the test dataset.\n",
    "        criterion (nn.Module): Loss function to calculate test loss.\n",
    "\n",
    "    Returns:\n",
    "        test_loss (float): Average loss on the test set.\n",
    "        test_accuracy (float): Accuracy on the test set.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    test_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            test_corrects += torch.sum(preds == labels.data)\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    test_loss = test_loss / total_samples\n",
    "    test_accuracy = test_corrects.double() / total_samples\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f}')\n",
    "    return test_loss, test_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obRcxvjraDi1"
   },
   "source": [
    "### definition of loss function\n",
    "In this class we define a variation of CrossEntropyLoss which also incorporates label smoothing, which is a regularization technique that adjusts the target labels during training by distributing a small portion of the target's \"probability mass\" to other classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:38:48.017497Z",
     "iopub.status.busy": "2024-12-11T14:38:48.017100Z",
     "iopub.status.idle": "2024-12-11T14:38:48.032693Z",
     "shell.execute_reply": "2024-12-11T14:38:48.031358Z",
     "shell.execute_reply.started": "2024-12-11T14:38:48.017460Z"
    },
    "id": "XVFHHvsLaDi2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Label Smoothing Cross Entropy Loss\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # Get number of classes\n",
    "        num_classes = pred.size(1)\n",
    "\n",
    "        # Create smoothed labels\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (num_classes - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), 1.0 - self.smoothing)\n",
    "\n",
    "        return torch.mean(torch.sum(-true_dist * F.log_softmax(pred, dim=1), dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMo58tRnaDi2"
   },
   "source": [
    "### definition of the network\n",
    "The following is a CNN which is called EnhancedCNN because its predecessor, the SimpleCNN, of course was extremely basic and didn't perform as well.\n",
    "This network uses 2d convolutions, with 64, 128, 256, 512 filters. We choose the convolutional layers because it was requested by the task assignment (aka, build a convolutional neural network). The activation function is ReLU.\n",
    "\n",
    "We also use batch normalization and max pooling after the convolutional layers. Batch normalization is used specifically because it speeds up convergence and improves generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:38:48.034862Z",
     "iopub.status.busy": "2024-12-11T14:38:48.034377Z",
     "iopub.status.idle": "2024-12-11T14:38:48.051654Z",
     "shell.execute_reply": "2024-12-11T14:38:48.049848Z",
     "shell.execute_reply.started": "2024-12-11T14:38:48.034812Z"
    },
    "id": "F2C6MADeaDi2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EnhancedCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EnhancedCNN, self).__init__()\n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Second convolutional block\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(256)\n",
    "        self.batchnorm4 = nn.BatchNorm2d(512)\n",
    "\n",
    "        # Global Average Pooling layer instead of flattening\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # Output size of 1x1\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(512, 1024)  # Adjust based on the output from the global avg pooling\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "        # Regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = self.pool(F.relu(self.batchnorm1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.batchnorm2(self.conv2(x))))\n",
    "\n",
    "        # Second block\n",
    "        x = self.pool(F.relu(self.batchnorm3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.batchnorm4(self.conv4(x))))\n",
    "\n",
    "        # Global Average Pooling\n",
    "        x = self.global_avg_pool(x)  # Output size becomes [batch_size, 512, 1, 1]\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor to [batch_size, 512]\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dD8TKGeaDi2"
   },
   "source": [
    "### Training the model\n",
    "As criterion we are using the **LabelSmoothingLoss** priorly defined, with smoothing parameter = 0.1. As an optimizer we are using **Adam**, which optimizes the model's weights in order to minimize the loss function. The parameter lr indicates the learning rate and sets the initial learning rate. Then the learning rate is adjusted via a scheduler, **StepLR** which dynamically adjusts the learning rate during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "execution": {
     "iopub.execute_input": "2024-12-11T14:38:48.053738Z",
     "iopub.status.busy": "2024-12-11T14:38:48.053377Z",
     "iopub.status.idle": "2024-12-11T15:03:02.292883Z",
     "shell.execute_reply": "2024-12-11T15:03:02.289898Z",
     "shell.execute_reply.started": "2024-12-11T14:38:48.053704Z"
    },
    "id": "btAzLgUAaDi2",
    "outputId": "59be1e85-b767-4fed-ff7d-0117ec686489",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "num_classes = train_dataset.get_num_classes()\n",
    "model = EnhancedCNN(num_classes).to(device)\n",
    "criterion = LabelSmoothingLoss(smoothing=0.1)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "\n",
    "# Train the model with early stopping\n",
    "trained_model, train_loss_arr, train_acc_arr, val_loss_arr, val_acc_arr = train_model_with_early_stopping_and_scheduler(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=30, patience=5)\n",
    "\n",
    "# Save the best model\n",
    "torch.save(trained_model.state_dict(), 'best_grocery_cnn.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q74-g1hlaDi3"
   },
   "source": [
    "### plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T15:03:02.957652Z",
     "iopub.status.busy": "2024-12-11T15:03:02.957309Z",
     "iopub.status.idle": "2024-12-11T15:03:32.497857Z",
     "shell.execute_reply": "2024-12-11T15:03:32.496715Z",
     "shell.execute_reply.started": "2024-12-11T15:03:02.957617Z"
    },
    "id": "j5WiUP0uaDi3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_grocery_cnn.pth'))\n",
    "model.to(device)\n",
    "test_loss, test_accuracy = evaluate_model_on_test_set(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkWEqSPoUIL3"
   },
   "source": [
    "## Part 2: fine-tune an existing network\n",
    "\n",
    "Your goal is to fine-tune a pretrained **ResNet-18** model on `GroceryStoreDataset`. Use the implementation provided by PyTorch, do not implement it yourselves! (i.e. exactly what you **could not** do in the first part of the assignment). Specifically, you must use the PyTorch ResNet-18 model pretrained on ImageNet-1K (V1). Divide your fine-tuning into two parts:\n",
    "\n",
    "1. First, fine-tune the Resnet-18 with the same training hyperparameters you used for your best model in the first part of the assignment.\n",
    "1. Then, tweak the training hyperparameters in order to increase the accuracy on the validation split of `GroceryStoreDataset`. Justify your choices by analyzing the training plots and/or citing sources that guided you in your decisions (papers, blog posts, YouTube videos, or whatever else you find enlightening). You should consider yourselves satisfied once you obtain a classification accuracy on the **validation** split **between 80 and 90%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pQ2_UZ5wo1z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xA_7QRNaDi4"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TFK0rQKaDi4"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import copy\n",
    "from torchvision import transforms\n",
    "\n",
    "# Step 1: Define device\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Step 2: Load pretrained ResNet-18 model\n",
    "#resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)  # Load pretrained ResNet-18\n",
    "\n",
    "# Step 3: Modify the classifier to fit the GroceryStoreDataset\n",
    "#num_classes = train_dataset.get_num_classes()  # Dynamically fetch the correct number of classes\n",
    "#resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n",
    "#resnet18 = resnet18.to(device)\n",
    "\n",
    "# Step 4: Define training and validation transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Step 5: Load datasets and dataloaders\n",
    "train_dataset = GroceryStoreDataset(split='train', transform=train_transforms)\n",
    "val_dataset = GroceryStoreDataset(split='val', transform=val_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# Step 2: Load pretrained ResNet-18 model\n",
    "resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Step 3: Modify the classifier to fit the GroceryStoreDataset\n",
    "num_classes = train_dataset.get_num_classes()  # Dynamically fetch the correct number of classes\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet18 = resnet18.to(device)\n",
    "\n",
    "# Step 6: Define the loss function, optimizer, and learning rate scheduler\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(resnet18.parameters(), lr=0.001)\n",
    "#scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "criterion = LabelSmoothingLoss(smoothing=0.1)\n",
    "optimizer = optim.Adam(resnet18.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "\n",
    "# Train the ResNet-18 model\n",
    "resnet18, train_loss_arr, train_acc_arr, val_loss_arr, val_acc_arr = train_model_with_early_stopping_and_scheduler(\n",
    "    model=resnet18,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=30,\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "torch.save(resnet18.state_dict(), 'fine_tuned_resnet18.pth')\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "TRbcsRb_zvZn"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch import Tensor\n",
    "from torchsummary import summary\n",
    "from torch.optim import Adam, lr_scheduler, SGD\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms as T\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "def get_data_loaders(train_transform, val_test_transform, batch_size=32):\n",
    "\n",
    "    train_dataset = GroceryStoreDataset(split='train', transform=train_transform)\n",
    "    val_dataset = GroceryStoreDataset(split='val', transform=val_test_transform)\n",
    "    test_dataset = GroceryStoreDataset(split='test', transform=val_test_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "train_loader, val_loader, _ = get_data_loaders(train_transform=train_transforms, val_test_transform=val_transforms)\n",
    "\n",
    "all_models = {}\n",
    "\n",
    "def get_model():\n",
    "    model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "DzLMDjIw0v_R"
   },
   "outputs": [],
   "source": [
    "model = get_model().to(device)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'fc' in name:  # If it's part of the fully connected layer\n",
    "        param.requires_grad = True  # Keep it trainable\n",
    "    else:\n",
    "        param.requires_grad = False  # Freeze all other layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "VcVhD6dz0xdH"
   },
   "outputs": [],
   "source": [
    "criterion = LabelSmoothingLoss(smoothing=0.1)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "num_epochs = 30\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5L3lokTY1FqC",
    "outputId": "6eca497d-c624-47f1-bd57-3777fe2f8da6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Start of training\n",
      "Epoch 1/30\n",
      "Training Loss: 2.7137 Acc: 0.3761\n",
      "Validation Loss: 2.2171 Acc: 0.4831\n",
      "--------------------\n",
      "Epoch 2/30\n",
      "Training Loss: 1.7220 Acc: 0.7011\n",
      "Validation Loss: 1.8677 Acc: 0.6318\n",
      "--------------------\n",
      "Epoch 3/30\n",
      "Training Loss: 1.4143 Acc: 0.8311\n",
      "Validation Loss: 1.7630 Acc: 0.6622\n",
      "--------------------\n",
      "Epoch 4/30\n",
      "Training Loss: 1.2798 Acc: 0.8705\n",
      "Validation Loss: 1.6843 Acc: 0.6622\n",
      "--------------------\n",
      "Epoch 5/30\n",
      "Training Loss: 1.1754 Acc: 0.9076\n",
      "Validation Loss: 1.6792 Acc: 0.6486\n",
      "--------------------\n",
      "Epoch 6/30\n",
      "Training Loss: 1.1369 Acc: 0.9159\n",
      "Validation Loss: 1.6102 Acc: 0.7027\n",
      "--------------------\n",
      "Epoch 7/30\n",
      "Training Loss: 1.0855 Acc: 0.9386\n",
      "Validation Loss: 1.6301 Acc: 0.7196\n",
      "--------------------\n",
      "Epoch 8/30\n",
      "Training Loss: 1.0657 Acc: 0.9386\n",
      "Validation Loss: 1.5484 Acc: 0.7162\n",
      "--------------------\n",
      "Epoch 9/30\n",
      "Training Loss: 1.0384 Acc: 0.9451\n",
      "Validation Loss: 1.5599 Acc: 0.7128\n",
      "--------------------\n",
      "Epoch 10/30\n",
      "Training Loss: 1.0186 Acc: 0.9530\n",
      "Validation Loss: 1.5497 Acc: 0.7432\n",
      "--------------------\n",
      "Epoch 11/30\n",
      "Training Loss: 0.9937 Acc: 0.9625\n",
      "Validation Loss: 1.5423 Acc: 0.7230\n",
      "--------------------\n",
      "Epoch 12/30\n",
      "Training Loss: 0.9900 Acc: 0.9576\n",
      "Validation Loss: 1.5450 Acc: 0.7264\n",
      "--------------------\n",
      "Epoch 13/30\n",
      "Training Loss: 0.9756 Acc: 0.9720\n",
      "Validation Loss: 1.5415 Acc: 0.7196\n",
      "--------------------\n",
      "Epoch 14/30\n",
      "Training Loss: 0.9905 Acc: 0.9678\n",
      "Validation Loss: 1.5304 Acc: 0.7230\n",
      "--------------------\n",
      "Epoch 15/30\n",
      "Training Loss: 0.9873 Acc: 0.9705\n",
      "Validation Loss: 1.5355 Acc: 0.7264\n",
      "Early stopping triggered\n",
      "Best val Acc: 0.7432\n"
     ]
    }
   ],
   "source": [
    "model_save_path='resnet18-v1.pth'\n",
    "\n",
    "trained_model, train_loss_arr, train_acc_arr, val_loss_arr, val_acc_arr = train_model_with_early_stopping_and_scheduler(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=num_epochs,\n",
    "    patience=patience\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctWIVCEYaDi5"
   },
   "source": [
    "#### Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gHe_LdKaDi5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Freeze all layers except the final one\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in resnet18.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Criterion, Optimizer, and Scheduler\n",
    "#criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss instead of Label Smoothing\n",
    "criterion = LabelSmoothingLoss(smoothing=0.1)\n",
    "\n",
    "optimizer = optim.Adam([\n",
    "    {'params': resnet18.fc.parameters(), 'lr': 1e-3},  # Higher LR for final layer\n",
    "    {'params': resnet18.layer4.parameters(), 'lr': 1e-4}])  # Lower LR for pretrained layers\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Train model with existing function\n",
    "trained_resnet, train_loss_arr, train_acc_arr, val_loss_arr, val_acc_arr = train_model_with_early_stopping_and_scheduler(\n",
    "    model=resnet18,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=30,\n",
    "    patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BjG3bWbIqVBD"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Unfreeze the final few layers\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = True  # Unfreeze all layers\n",
    "\n",
    "optimizer = optim.Adam([\n",
    "    {'params': resnet18.fc.parameters(), 'lr': 1e-3},       # Higher learning rate for final layer\n",
    "    {'params': resnet18.layer4.parameters(), 'lr': 1e-4},   # Intermediate for higher layers\n",
    "    {'params': resnet18.layer3.parameters(), 'lr': 1e-5}    # Lower for earlier layers\n",
    "])\n",
    "# Learning rate scheduler to decay LR over time\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)  # Decay LR more frequently for fine-tuning\n",
    "\n",
    "# Redefine train_model function for better monitoring (keep track of the train/val loss and accuracy per epoch)\n",
    "\n",
    "trained_resnet, train_loss_arr, train_acc_arr, val_loss_arr, val_acc_arr = train_model_with_early_stopping_and_scheduler(\n",
    "    model=resnet18,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=20,  # Adjusted epochs for fine-tuning\n",
    "    patience=10  # Patience for early stopping\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42NMzHPUxKs5"
   },
   "source": [
    "New try, with new criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J36RqLe3wq_n"
   },
   "outputs": [],
   "source": [
    "# Freeze all layers except the final one\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in resnet18.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Criterion, Optimizer, and Scheduler\n",
    "criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss instead of Label Smoothing\n",
    "#criterion = LabelSmoothingLoss(smoothing=0.1)\n",
    "\n",
    "optimizer = optim.Adam([\n",
    "    {'params': resnet18.fc.parameters(), 'lr': 1e-3},  # Higher LR for final layer\n",
    "    {'params': resnet18.layer4.parameters(), 'lr': 1e-4}])  # Lower LR for pretrained layers\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Train model with existing function\n",
    "trained_resnet, train_loss_arr, train_acc_arr, val_loss_arr, val_acc_arr = train_model_with_early_stopping_and_scheduler(\n",
    "    model=resnet18,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=30,\n",
    "    patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3y-kLLhy4Hm"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Unfreeze the final few layers\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = True  # Unfreeze all layers\n",
    "\n",
    "optimizer = optim.Adam([\n",
    "    {'params': resnet18.fc.parameters(), 'lr': 1e-3},       # Higher learning rate for final layer\n",
    "    {'params': resnet18.layer4.parameters(), 'lr': 1e-4},   # Intermediate for higher layers\n",
    "    {'params': resnet18.layer3.parameters(), 'lr': 1e-5}    # Lower for earlier layers\n",
    "])\n",
    "# Learning rate scheduler to decay LR over time\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)  # Decay LR more frequently for fine-tuning\n",
    "\n",
    "# Redefine train_model function for better monitoring (keep track of the train/val loss and accuracy per epoch)\n",
    "\n",
    "trained_resnet, train_loss_arr, train_acc_arr, val_loss_arr, val_acc_arr = train_model_with_early_stopping_and_scheduler(\n",
    "    model=resnet18,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=20,  # Adjusted epochs for fine-tuning\n",
    "    patience=10  # Patience for early stopping\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6274954,
     "sourceId": 10161805,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
