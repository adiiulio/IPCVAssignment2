{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Assignment Module 2: Product Classification\n\nThe goal of this assignment is to implement a neural network that classifies smartphone pictures of products found in grocery stores. The assignment will be divided into two parts: first, you will be asked to implement from scratch your own neural network for image classification; then, you will fine-tune a pretrained network provided by PyTorch.\n","metadata":{"id":"MNBgGYg_lpVN"}},{"cell_type":"markdown","source":"## Preliminaries: the dataset\n\nThe dataset you will be using contains natural images of products taken with a smartphone camera in different grocery stores:\n\n<p align=\"center\">\n  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Granny-Smith.jpg\" width=\"150\">\n  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Pink-Lady.jpg\" width=\"150\">\n  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Lemon.jpg\" width=\"150\">\n  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Banana.jpg\" width=\"150\">\n  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Vine-Tomato.jpg\" width=\"150\">\n</p>\n<p align=\"center\">\n  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Yellow-Onion.jpg\" width=\"150\">\n  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Green-Bell-Pepper.jpg\" width=\"150\">\n  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Arla-Standard-Milk.jpg\" width=\"150\">\n  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Oatly-Natural-Oatghurt.jpg\" width=\"150\">\n  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Alpro-Fresh-Soy-Milk.jpg\" width=\"150\">\n</p>\n\nThe products belong to the following 43 classes:\n```\n0.  Apple\n1.  Avocado\n2.  Banana\n3.  Kiwi\n4.  Lemon\n5.  Lime\n6.  Mango\n7.  Melon\n8.  Nectarine\n9.  Orange\n10. Papaya\n11. Passion-Fruit\n12. Peach\n13. Pear\n14. Pineapple\n15. Plum\n16. Pomegranate\n17. Red-Grapefruit\n18. Satsumas\n19. Juice\n20. Milk\n21. Oatghurt\n22. Oat-Milk\n23. Sour-Cream\n24. Sour-Milk\n25. Soyghurt\n26. Soy-Milk\n27. Yoghurt\n28. Asparagus\n29. Aubergine\n30. Cabbage\n31. Carrots\n32. Cucumber\n33. Garlic\n34. Ginger\n35. Leek\n36. Mushroom\n37. Onion\n38. Pepper\n39. Potato\n40. Red-Beet\n41. Tomato\n42. Zucchini\n```\n\nThe dataset is split into training (`train`), validation (`val`), and test (`test`) set.","metadata":{"id":"dVTQUJ4uYH1w"}},{"cell_type":"markdown","source":"The following code cells download the dataset and define a `torch.utils.data.Dataset` class to access it. This `Dataset` class will be the starting point of your assignment: use it in your own code and build everything else around it.","metadata":{"id":"1pdrmJRnJPd8"}},{"cell_type":"code","source":"!git clone https://github.com/marcusklasson/GroceryStoreDataset.git","metadata":{"id":"POMX_3x-_bZI","execution":{"iopub.status.busy":"2024-10-20T11:14:24.646351Z","iopub.execute_input":"2024-10-20T11:14:24.646896Z","iopub.status.idle":"2024-10-20T11:14:31.292376Z","shell.execute_reply.started":"2024-10-20T11:14:24.646839Z","shell.execute_reply":"2024-10-20T11:14:31.291136Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Cloning into 'GroceryStoreDataset'...\nremote: Enumerating objects: 6559, done.\u001b[K\nremote: Counting objects: 100% (266/266), done.\u001b[K\nremote: Compressing objects: 100% (231/231), done.\u001b[K\nremote: Total 6559 (delta 45), reused 35 (delta 35), pack-reused 6293 (from 1)\u001b[K\nReceiving objects: 100% (6559/6559), 116.26 MiB | 34.76 MiB/s, done.\nResolving deltas: 100% (275/275), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from pathlib import Path\nfrom PIL import Image\nfrom torch import Tensor\nfrom torch.utils.data import Dataset\nfrom typing import List, Tuple","metadata":{"id":"hiF8xGEYlsu8","execution":{"iopub.status.busy":"2024-10-20T11:14:31.294838Z","iopub.execute_input":"2024-10-20T11:14:31.295874Z","iopub.status.idle":"2024-10-20T11:14:34.532382Z","shell.execute_reply.started":"2024-10-20T11:14:31.295827Z","shell.execute_reply":"2024-10-20T11:14:34.531110Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class GroceryStoreDataset(Dataset):\n\n    def __init__(self, split: str, transform=None) -> None:\n        super().__init__()\n\n        self.root = Path(\"GroceryStoreDataset/dataset\")\n        self.split = split\n        self.paths, self.labels = self.read_file()\n\n        self.transform = transform\n\n    def __len__(self) -> int:\n        return len(self.labels)\n\n    def __getitem__(self, idx) -> Tuple[Tensor, int]:\n        img = Image.open(self.root / self.paths[idx])\n        label = self.labels[idx]\n\n        if self.transform:\n            img = self.transform(img)\n\n        return img, label\n\n    def read_file(self) -> Tuple[List[str], List[int]]:\n        paths = []\n        labels = []\n\n        with open(self.root / f\"{self.split}.txt\") as f:\n            for line in f:\n                # path, fine-grained class, coarse-grained class\n                path, _, label = line.replace(\"\\n\", \"\").split(\", \")\n                paths.append(path), labels.append(int(label))\n\n        return paths, labels\n\n    def get_num_classes(self) -> int:\n        return max(self.labels) + 1","metadata":{"id":"jROSO2qVDxdD","execution":{"iopub.status.busy":"2024-10-20T11:14:34.533753Z","iopub.execute_input":"2024-10-20T11:14:34.534269Z","iopub.status.idle":"2024-10-20T11:14:34.547906Z","shell.execute_reply.started":"2024-10-20T11:14:34.534230Z","shell.execute_reply":"2024-10-20T11:14:34.546779Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Part 1: design your own network\n\nYour goal is to implement a convolutional neural network for image classification and train it on `GroceryStoreDataset`. You should consider yourselves satisfied once you obtain a classification accuracy on the **validation** split of **around 60%**. You are free to achieve that however you want, except for a few rules you must follow:\n\n- You **cannot** simply instantiate an off-the-self PyTorch network. Instead, you must construct your network as a composition of existing PyTorch layers. In more concrete terms, you can use e.g. `torch.nn.Linear`, but you **cannot** use e.g. `torchvision.models.alexnet`.\n\n- Justify every *design choice* you make. Design choices include network architecture, training hyperparameters, and, possibly, dataset preprocessing steps. You can either (i) start from the simplest convolutional network you can think of and add complexity one step at a time, while showing how each step gets you closer to the target ~60%, or (ii) start from a model that is already able to achieve the desired accuracy and show how, by removing some of its components, its performance drops (i.e. an *ablation study*). You can *show* your results/improvements however you want: training plots, console-printed values or tables, or whatever else your heart desires: the clearer, the better.\n\nDon't be too concerned with your network performance: the ~60% is just to give you an idea of when to stop. Keep in mind that a thoroughly justified model with lower accuracy will be rewarded **more** points than a poorly experimentally validated model with higher accuracy.","metadata":{"id":"yBch3dpwNSsW"}},{"cell_type":"code","source":"# part one, data preprocessing and augmentation\nimport torchvision.transforms as transforms\n\n# Data augmentation and normalization\ntrain_transforms = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T11:14:34.550038Z","iopub.execute_input":"2024-10-20T11:14:34.550576Z","iopub.status.idle":"2024-10-20T11:14:36.004623Z","shell.execute_reply.started":"2024-10-20T11:14:34.550524Z","shell.execute_reply":"2024-10-20T11:14:36.003594Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SimpleCNN(nn.Module):\n    def __init__(self, num_classes):\n        super(SimpleCNN, self).__init__()\n        \n        # First convolutional block\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        # Second convolutional block\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(256 * 4 * 4, 512)  # Adjusted input size\n        self.fc2 = nn.Linear(512, num_classes)\n        \n        # Dropout for regularization\n        self.dropout = nn.Dropout(0.5)\n        \n    def forward(self, x):\n        # First conv block\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        \n        # Second conv block\n        x = self.pool(F.relu(self.conv3(x)))\n        x = self.pool(F.relu(self.conv4(x)))\n        \n        \n        # Flattening the output from convolutional layers\n        x = x.view(x.size(0), -1)  # Dynamically flatten\n        \n        # Fully connected layers\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)  # Adding dropout\n        x = self.fc2(x)\n        \n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T11:14:36.008045Z","iopub.execute_input":"2024-10-20T11:14:36.008991Z","iopub.status.idle":"2024-10-20T11:14:36.021459Z","shell.execute_reply.started":"2024-10-20T11:14:36.008935Z","shell.execute_reply":"2024-10-20T11:14:36.020142Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class EnhancedCNN(nn.Module):\n    def __init__(self, num_classes):\n        super(EnhancedCNN, self).__init__()\n        # First convolutional block\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.batchnorm1 = nn.BatchNorm2d(64)\n        self.batchnorm2 = nn.BatchNorm2d(128)\n\n        # Second convolutional block\n        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n        self.batchnorm3 = nn.BatchNorm2d(256)\n        self.batchnorm4 = nn.BatchNorm2d(512)\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(512 * 4 * 4, 1024)  # Adjust based on the output size from conv layers\n        self.fc2 = nn.Linear(1024, num_classes)\n\n        # Regularization\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        # First block\n        x = self.pool(F.relu(self.batchnorm1(self.conv1(x))))\n        x = self.pool(F.relu(self.batchnorm2(self.conv2(x))))\n\n        # Second block\n        x = self.pool(F.relu(self.batchnorm3(self.conv3(x))))\n        x = self.pool(F.relu(self.batchnorm4(self.conv4(x))))\n\n        # Flatten\n        x = x.view(x.size(0), -1)  # Flatten the tensor\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T11:14:36.022913Z","iopub.execute_input":"2024-10-20T11:14:36.023333Z","iopub.status.idle":"2024-10-20T11:14:36.041233Z","shell.execute_reply.started":"2024-10-20T11:14:36.023292Z","shell.execute_reply":"2024-10-20T11:14:36.040127Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20):\n    model.train()\n    \n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        # Training loop\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n            \n            # Track training loss\n            running_loss += loss.item()\n            \n            # Track training accuracy\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n        \n        # Calculate average training loss and accuracy for the epoch\n        epoch_loss = running_loss / len(train_loader)\n        epoch_acc = 100 * correct / total\n        \n        # Now do validation at the end of the epoch\n        val_loss, val_acc = validate_model(model, val_loader, criterion)\n        \n        # Print out the results for the current epoch\n        print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n              f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}% | \"\n              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n\n    return model\n\ndef validate_model(model, val_loader, criterion):\n    model.eval()  # Set the model to evaluation mode (disable dropout, etc.)\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():  # We don't need gradients during validation\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            # Track validation loss\n            running_loss += loss.item()\n            \n            # Track validation accuracy\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    # Calculate average validation loss and accuracy\n    val_loss = running_loss / len(val_loader)\n    val_acc = 100 * correct / total\n    \n    model.train()  # Set back to train mode after validation\n    return val_loss, val_acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T11:14:36.042747Z","iopub.execute_input":"2024-10-20T11:14:36.043146Z","iopub.status.idle":"2024-10-20T11:14:36.059046Z","shell.execute_reply.started":"2024-10-20T11:14:36.043106Z","shell.execute_reply":"2024-10-20T11:14:36.057913Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import copy\n\ndef train_model_with_early_stopping(model, train_loader, val_loader, criterion, optimizer, num_epochs=20, patience=5):\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    epochs_no_improve = 0\n    early_stop = False\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        \n        # Training phase\n        model.train()\n        running_loss = 0.0\n        running_corrects = 0\n        \n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            _, preds = torch.max(outputs, 1)\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n        \n        epoch_loss = running_loss / len(train_loader.dataset)\n        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n\n        print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        val_corrects = 0\n        \n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                _, preds = torch.max(outputs, 1)\n                val_loss += loss.item() * inputs.size(0)\n                val_corrects += torch.sum(preds == labels.data)\n        \n        val_loss = val_loss / len(val_loader.dataset)\n        val_acc = val_corrects.double() / len(val_loader.dataset)\n\n        print(f'Validation Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n\n        # Check if early stopping condition is met\n        if val_acc > best_acc:\n            best_acc = val_acc\n            best_model_wts = copy.deepcopy(model.state_dict())\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n\n        if epochs_no_improve >= patience:\n            print(\"Early stopping triggered\")\n            early_stop = True\n            break\n\n        print(\"-\" * 20)\n\n    # Restore best model weights\n    print(f\"Best val Acc: {best_acc:.4f}\")\n    model.load_state_dict(best_model_wts)\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T11:14:36.060631Z","iopub.execute_input":"2024-10-20T11:14:36.060989Z","iopub.status.idle":"2024-10-20T11:14:36.077800Z","shell.execute_reply.started":"2024-10-20T11:14:36.060950Z","shell.execute_reply":"2024-10-20T11:14:36.076764Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"#part 3, put everything together\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nimport torch\n\n# Step 1: Define the device (GPU or CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the datasets\ntrain_dataset = GroceryStoreDataset(split='train', transform=train_transforms)\nval_dataset = GroceryStoreDataset(split='val', transform=val_transforms)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Initialize the model, loss function, and optimizer\nnum_classes = train_dataset.get_num_classes()\nmodel = EnhancedCNN(num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Train the model with early stopping\ntrained_model = train_model_with_early_stopping(model, train_loader, val_loader, criterion, optimizer, num_epochs=30, patience=5)\n\n# Save the best model\ntorch.save(trained_model.state_dict(), 'best_grocery_cnn.pth')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T11:14:36.079324Z","iopub.execute_input":"2024-10-20T11:14:36.080311Z","iopub.status.idle":"2024-10-20T11:52:37.724112Z","shell.execute_reply.started":"2024-10-20T11:14:36.080258Z","shell.execute_reply":"2024-10-20T11:52:37.722950Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\nTraining Loss: 3.7434 Acc: 0.1803\nValidation Loss: 2.9211 Acc: 0.2331\n--------------------\nEpoch 2/30\nTraining Loss: 2.5432 Acc: 0.2492\nValidation Loss: 2.6292 Acc: 0.2027\n--------------------\nEpoch 3/30\nTraining Loss: 2.3151 Acc: 0.2977\nValidation Loss: 2.3659 Acc: 0.2804\n--------------------\nEpoch 4/30\nTraining Loss: 2.0840 Acc: 0.3443\nValidation Loss: 2.4433 Acc: 0.2838\n--------------------\nEpoch 5/30\nTraining Loss: 1.8121 Acc: 0.4015\nValidation Loss: 2.3703 Acc: 0.2534\n--------------------\nEpoch 6/30\nTraining Loss: 1.7239 Acc: 0.4481\nValidation Loss: 2.1665 Acc: 0.3581\n--------------------\nEpoch 7/30\nTraining Loss: 1.6025 Acc: 0.4697\nValidation Loss: 2.1315 Acc: 0.3243\n--------------------\nEpoch 8/30\nTraining Loss: 1.4526 Acc: 0.5178\nValidation Loss: 2.3859 Acc: 0.3784\n--------------------\nEpoch 9/30\nTraining Loss: 1.3749 Acc: 0.5439\nValidation Loss: 2.1648 Acc: 0.3919\n--------------------\nEpoch 10/30\nTraining Loss: 1.2772 Acc: 0.5761\nValidation Loss: 1.9490 Acc: 0.4020\n--------------------\nEpoch 11/30\nTraining Loss: 1.2851 Acc: 0.5765\nValidation Loss: 1.7969 Acc: 0.4561\n--------------------\nEpoch 12/30\nTraining Loss: 1.1298 Acc: 0.6129\nValidation Loss: 1.8549 Acc: 0.4797\n--------------------\nEpoch 13/30\nTraining Loss: 1.0941 Acc: 0.6220\nValidation Loss: 1.9840 Acc: 0.4257\n--------------------\nEpoch 14/30\nTraining Loss: 1.0221 Acc: 0.6598\nValidation Loss: 2.0803 Acc: 0.4628\n--------------------\nEpoch 15/30\nTraining Loss: 1.0045 Acc: 0.6598\nValidation Loss: 1.7922 Acc: 0.4764\n--------------------\nEpoch 16/30\nTraining Loss: 0.9729 Acc: 0.6716\nValidation Loss: 2.6573 Acc: 0.4223\n--------------------\nEpoch 17/30\nTraining Loss: 0.8622 Acc: 0.7030\nValidation Loss: 1.9188 Acc: 0.4966\n--------------------\nEpoch 18/30\nTraining Loss: 0.8864 Acc: 0.7076\nValidation Loss: 1.9393 Acc: 0.4865\n--------------------\nEpoch 19/30\nTraining Loss: 0.8449 Acc: 0.7167\nValidation Loss: 1.9193 Acc: 0.4865\n--------------------\nEpoch 20/30\nTraining Loss: 0.7816 Acc: 0.7330\nValidation Loss: 1.8146 Acc: 0.5473\n--------------------\nEpoch 21/30\nTraining Loss: 0.7494 Acc: 0.7443\nValidation Loss: 2.4508 Acc: 0.4155\n--------------------\nEpoch 22/30\nTraining Loss: 0.7538 Acc: 0.7386\nValidation Loss: 1.7633 Acc: 0.5236\n--------------------\nEpoch 23/30\nTraining Loss: 0.6754 Acc: 0.7765\nValidation Loss: 2.2868 Acc: 0.4764\n--------------------\nEpoch 24/30\nTraining Loss: 0.6273 Acc: 0.7818\nValidation Loss: 2.1430 Acc: 0.5304\n--------------------\nEpoch 25/30\nTraining Loss: 0.6717 Acc: 0.7701\nValidation Loss: 1.9790 Acc: 0.5642\n--------------------\nEpoch 26/30\nTraining Loss: 0.5677 Acc: 0.8148\nValidation Loss: 2.1196 Acc: 0.5270\n--------------------\nEpoch 27/30\nTraining Loss: 0.5975 Acc: 0.7909\nValidation Loss: 2.0284 Acc: 0.5135\n--------------------\nEpoch 28/30\nTraining Loss: 0.5254 Acc: 0.8231\nValidation Loss: 2.0152 Acc: 0.5507\n--------------------\nEpoch 29/30\nTraining Loss: 0.5776 Acc: 0.8072\nValidation Loss: 2.3574 Acc: 0.4797\n--------------------\nEpoch 30/30\nTraining Loss: 0.5836 Acc: 0.8072\nValidation Loss: 2.4568 Acc: 0.4831\nEarly stopping triggered\nBest val Acc: 0.5642\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"#part 3, put everything together\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nimport torch\n\n# Step 1: Define the device (GPU or CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the datasets\ntrain_dataset = GroceryStoreDataset(split='train', transform=train_transforms)\nval_dataset = GroceryStoreDataset(split='val', transform=val_transforms)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Assuming train_dataset is initialized already\nnum_classes = train_dataset.get_num_classes()  # Get number of classes from the train dataset\n\n# Initialize the model, loss function, and optimizer\nmodel = SimpleCNN(num_classes).to(device)  # Use the new model\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Train the model\ntrained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20)\n\n# Save the trained model\ntorch.save(trained_model.state_dict(), 'grocery_cnn.pth')\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T11:52:37.726283Z","iopub.execute_input":"2024-10-20T11:52:37.726758Z","iopub.status.idle":"2024-10-20T12:00:50.855431Z","shell.execute_reply.started":"2024-10-20T11:52:37.726704Z","shell.execute_reply":"2024-10-20T12:00:50.854159Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/20] - Train Loss: 3.1770, Train Acc: 15.08% | Val Loss: 2.9381, Val Acc: 15.88%\nEpoch [2/20] - Train Loss: 2.5818, Train Acc: 23.67% | Val Loss: 2.7329, Val Acc: 25.68%\nEpoch [3/20] - Train Loss: 2.1707, Train Acc: 32.31% | Val Loss: 2.5510, Val Acc: 23.65%\nEpoch [4/20] - Train Loss: 1.7895, Train Acc: 43.33% | Val Loss: 2.3654, Val Acc: 30.41%\nEpoch [5/20] - Train Loss: 1.5029, Train Acc: 51.06% | Val Loss: 2.2544, Val Acc: 34.46%\nEpoch [6/20] - Train Loss: 1.3497, Train Acc: 56.21% | Val Loss: 2.6383, Val Acc: 31.08%\nEpoch [7/20] - Train Loss: 1.1612, Train Acc: 61.86% | Val Loss: 2.0967, Val Acc: 35.81%\nEpoch [8/20] - Train Loss: 0.9846, Train Acc: 66.67% | Val Loss: 2.3080, Val Acc: 41.89%\nEpoch [9/20] - Train Loss: 0.8945, Train Acc: 70.08% | Val Loss: 2.1547, Val Acc: 37.50%\nEpoch [10/20] - Train Loss: 0.8338, Train Acc: 72.08% | Val Loss: 2.1249, Val Acc: 36.82%\nEpoch [11/20] - Train Loss: 0.6934, Train Acc: 76.21% | Val Loss: 2.1793, Val Acc: 41.22%\nEpoch [12/20] - Train Loss: 0.6214, Train Acc: 79.05% | Val Loss: 2.4210, Val Acc: 37.84%\nEpoch [13/20] - Train Loss: 0.5907, Train Acc: 79.55% | Val Loss: 2.3104, Val Acc: 44.93%\nEpoch [14/20] - Train Loss: 0.5170, Train Acc: 82.42% | Val Loss: 2.8294, Val Acc: 45.27%\nEpoch [15/20] - Train Loss: 0.4673, Train Acc: 83.98% | Val Loss: 2.6217, Val Acc: 41.22%\nEpoch [16/20] - Train Loss: 0.4621, Train Acc: 84.96% | Val Loss: 2.8032, Val Acc: 43.58%\nEpoch [17/20] - Train Loss: 0.4170, Train Acc: 85.98% | Val Loss: 2.7490, Val Acc: 42.23%\nEpoch [18/20] - Train Loss: 0.3436, Train Acc: 88.03% | Val Loss: 2.8328, Val Acc: 45.61%\nEpoch [19/20] - Train Loss: 0.3497, Train Acc: 87.42% | Val Loss: 2.8537, Val Acc: 45.95%\nEpoch [20/20] - Train Loss: 0.2984, Train Acc: 90.34% | Val Loss: 3.3157, Val Acc: 41.89%\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Part 2: fine-tune an existing network\n\nYour goal is to fine-tune a pretrained **ResNet-18** model on `GroceryStoreDataset`. Use the implementation provided by PyTorch, do not implement it yourselves! (i.e. exactly what you **could not** do in the first part of the assignment). Specifically, you must use the PyTorch ResNet-18 model pretrained on ImageNet-1K (V1). Divide your fine-tuning into two parts:\n\n1. First, fine-tune the Resnet-18 with the same training hyperparameters you used for your best model in the first part of the assignment.\n1. Then, tweak the training hyperparameters in order to increase the accuracy on the validation split of `GroceryStoreDataset`. Justify your choices by analyzing the training plots and/or citing sources that guided you in your decisions (papers, blog posts, YouTube videos, or whatever else you find enlightening). You should consider yourselves satisfied once you obtain a classification accuracy on the **validation** split **between 80 and 90%**.","metadata":{"id":"gkWEqSPoUIL3"}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}